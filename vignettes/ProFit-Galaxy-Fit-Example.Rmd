---
title: "ProFit: Galaxy Fitting Example"
author: "Aaron Robotham & Dan Taranu"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ProFit Galaxy Fitting Example}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Prepare the test data

It is worth checking that you have the latest version of `ProFit`:

```{r, eval=FALSE}
library(devtools)
install_github('ICRAR/ProFit')
```

Next we load the libraries we need:

```{r}
library(knitr)
library(ProFit)
library(FITSio)
```

Next we load a table of data describing GAMA galaxies:

```{r}
data('ExampleInit', package="ProFit")
kable(head(ExampleInit, 10))
```

Now we can extract out the example files we have available for fitting by checking the contents of the directory containing the example FITS files:

```{r}
ExampleFiles=list.files(paste(.libPaths()[1],'/ProFit/inst/extdata/KiDS/',sep=''))
ExampleIDs=unlist(strsplit(ExampleFiles[grep('fitim',ExampleFiles)],'fitim.fits'))
ExampleIDs
```

There are 2 data source options: KiDS or SDSS (the galaxies are the same)

```{r}
datasource='KiDS'
```

There are 10 example galaxies included. Here we run example 1:

```{r}
useID=ExampleIDs[1]
image = readFITS(system.file("extdata", paste(datasource,'/',useID,'fitim.fits',sep=''),package="ProFit"))$imDat
mask = readFITS(system.file("extdata", paste(datasource,'/',useID,'mskim.fits',sep=''),package="ProFit"))$imDat
sigma = readFITS(system.file("extdata", paste(datasource,'/',useID,'sigma.fits',sep=''),package="ProFit"))$imDat
segim = readFITS(system.file("extdata", paste(datasource,'/',useID,'segim.fits',sep=''),package="ProFit"))$imDat
psf = readFITS(system.file("extdata", paste(datasource,'/',useID,'psfim.fits',sep=''),package="ProFit"))$imDat
```

Next we extract parameters for a very rough model (not meant to look too good yet):

```{r}
useIDnum=as.integer(strsplit(useID,'G')[[1]][2])
useloc=which(ExampleInit$CATAID==useIDnum)
```

## Setup the fitting data structures

For our initial model we treat component 1 as the putative bulge and component 2 as the putative disk. We are going to attempt a fit where the disk is forced to have nser=1 and the bulge has an axial ratio of 1.

```{r}
modellist=list(
  sersic=list(
    xcen= c(dim(image)[1]/2, dim(image)[1]/2),
    ycen= c(dim(image)[2]/2, dim(image)[2]/2),
    mag= c(ExampleInit$sersic.mag1[useloc], ExampleInit$sersic.mag2[useloc]),
    re= c(ExampleInit$sersic.re1[useloc], ExampleInit$sersic.re2[useloc])*
      if(datasource=='KiDS'){1}else{0.2/0.339},
    nser= c(ExampleInit$sersic.nser1[useloc], 1),  #Disk is initially nser=1
    ang= c(ExampleInit$sersic.ang2[useloc], ExampleInit$sersic.ang2[useloc]),
    axrat= c(1, ExampleInit$sersic.axrat2[useloc]),  #Bulge is initially axrat=1
    box=c(0, 0)
  )
)
modellist
```

The pure model (no PSF):

```{r, fig.width=5, fig.height=5}
magimage(profitMakeModel(modellist,dim=dim(image)))
```

The original image:

```{r, fig.width=5, fig.height=5}
magimage(image)
```

The convolved model (with PSF):

```{r, fig.width=5, fig.height=5}
magimage(profitMakeModel(modellist,dim=dim(image),psf=psf))
```

Next we define our list of what we want to fit (where TRUE means we will fit it later):

```{r}
tofit=list(
  sersic=list(
    xcen= c(TRUE,NA), #We fit for xcen and tie the two togther
    ycen= c(TRUE,NA), #We fit for ycen and tie the two togther
    mag= c(TRUE,TRUE), #Fit for both
    re= c(TRUE,TRUE), #Fit for both
    nser= c(TRUE,FALSE), #Fit for bulge
    ang= c(FALSE,TRUE), #Fit for disk
    axrat= c(FALSE,TRUE), #Fit for disk
    box= c(FALSE,FALSE) #Fit for neither
  )
)
```

Now we define what parameters should be fitted in log space:

```{r}
tolog=list(
  sersic=list(
    xcen= c(FALSE,FALSE),
    ycen= c(FALSE,FALSE),
    mag= c(FALSE,FALSE),
    re= c(TRUE,TRUE), #re is best fit in log space
    nser= c(TRUE,TRUE), #nser is best fit in log space
    ang= c(FALSE,FALSE),
    axrat= c(TRUE,TRUE), #axrat is best fit in log space
    box= c(FALSE,FALSE)
  )
)
```

Now we specify the prior functions. If the parameters are to be sampled in log space (above) then the priors will refer to dex not linear standard deviations. Priors should be specified in their unlogged state- the logging is done internally.

```{r}
sigmas=c(2,2,2,2,5,5,1,1,1,1,30,30,0.3,0.3,0.3,0.3)

priors=list(
  sersic=list(
    xcen=list(function(x){dnorm(x,0,sigmas[1],log=T)},function(x){dnorm(x,0,sigmas[2],
    log=T)}), # should have tight constraints on x and y
    ycen=list(function(x){dnorm(x,0,sigmas[3],log=T)},function(x){dnorm(x,0,sigmas[4],
    log=T)}), # should have tight constraints on x and y
    mag=list(function(x){dnorm(x,0,sigmas[5],log=T)},function(x){dnorm(x,0,sigmas[6],
    log=T)}), # 5 mag SD
    re=list(function(x){dnorm(x,0,sigmas[7],log=T)},function(x){dnorm(x,0,sigmas[8],
    log=T)}), # i.e. 1 dex in re is the SD
    nser=list(function(x){dnorm(x,0,sigmas[9],log=T)},function(x){dnorm(x,0,sigmas[10],
    log=T)}), # i.e. 1 dex in nser is the SD
    ang=list(function(x){dnorm(x,0,sigmas[11],log=T)},function(x){dnorm(x,0,sigmas[12],
    log=T)}), # very broad 30 deg ang SD
    axrat=list(function(x){dnorm(x,0,sigmas[13],log=T)},function(x){dnorm(x,0,sigmas[14],
    log=T)}), # i.e. 1 dex in axrat is the SD
    box=list(function(x){dnorm(x,0,sigmas[15],log=T)},function(x){dnorm(x,0,sigmas[16],
    log=T)})
  )
)
```

The hard intervals should also be specified in log space if relevant:

```{r}
lowers=c(0,0,0,0,10,10,0,0,-1,-1,-180,-180,-1,-1,-1,-1)
uppers=c(1e3,1e3,1e3,1e3,30,30,2,2,1.3,1.3,360,360,0,0,1,1)

intervals=list(
  sersic=list(
    xcen=list(function(x){interval(x,lowers[1],uppers[1],reflect=F)},
    function(x){interval(x,lowers[2],uppers[2],reflect=F)}),
    ycen=list(function(x){interval(x,lowers[3],uppers[3],reflect=F)},
    function(x){interval(x,lowers[4],uppers[4],reflect=F)}),
    mag=list(function(x){interval(x,lowers[5],uppers[5],reflect=F)},
    function(x){interval(x,lowers[6],uppers[6],reflect=F)}),
    re=list(function(x){interval(x,lowers[7],uppers[7],reflect=F)},
    function(x){interval(x,lowers[8],uppers[8],reflect=F)}),
    nser=list(function(x){interval(x,lowers[9],uppers[9],reflect=F)},
    function(x){interval(x,lowers[10],uppers[10],reflect=F)}),
    ang=list(function(x){interval(x,lowers[11],uppers[11],reflect=F)},
    function(x){interval(x,lowers[12],uppers[12],reflect=F)}),
    axrat=list(function(x){interval(x,lowers[13],uppers[13],reflect=F)},
    function(x){interval(x,lowers[14],uppers[14],reflect=F)}),
    box=list(function(x){interval(x,lowers[15],uppers[15],reflect=F)},
    function(x){interval(x,lowers[16],uppers[16],reflect=F)})
  )
)
```

Setup the data structure we need for optimisation, taking a few seconds to find the optimal convolution method:

```{r}
Data=profitSetupData(image=image, mask=mask, sigma=sigma, segim=segim, psf=psf,
  modellist=modellist, tofit=tofit, tolog=tolog, priors=priors, intervals=intervals,
  magzero=0, algo.func='optim', like.func = "t", verbose=TRUE)
```

This produces a fairly complex R object, but with all the bits we need for fitting, e.g. (notice the tolog parameteres are now logged):

```{r}
Data$init
```

These are the parameters we wish to fit for, and we take the initial guesses from the model list we provided before.

We can test how things currently look (we get an output because we set verbose=TRUE earlier):

```{r,  fig.width=7, fig.height=3}
profitLikeModel(parm=Data$init, Data=Data, makeplots=TRUE)
```

## Do some fitting

First try `optim` L-BFGS-B:

```{r, eval=FALSE}
optimfit=optim(Data$init, profitLikeModel, method='L-BFGS-B', Data=Data, rough=TRUE,
lower=lowers[which(unlist(tofit))], upper=uppers[which(unlist(tofit))],
control=list(fnscale=-1,parscale=sigmas[which(unlist(tofit))]))
```

The best `optim` L-BFGS-B fit is given by:

```{r, eval=FALSE}
optimfit$par
```

Check it out:

```{r, eval=FALSE}
profitLikeModel(optimfit$par,Data,makeplots=TRUE,whichcomponents=list(sersic=1))
profitLikeModel(optimfit$par,Data,makeplots=TRUE,whichcomponents=list(sersic=2))
profitLikeModel(optimfit$par,Data,makeplots=TRUE,whichcomponents=list(sersic='all'))
```

```{r, eval=FALSE}
modeloptim=profitRemakeModelList(optimfit$par,Data$modellist,Data$tofit,Data$tolog)
profitEllipsePlot(Data,modeloptim,pixscale=0.2,FWHM=0.5,SBlim=26)
```

Now we can try a `LaplaceApproximation` fit. This should take a few minutes and might stall briefly when computing models near the intervals defined above:

```{r, eval=FALSE}
library(LaplacesDemon)
Data$algo.func = "LA"
LAfit=LaplaceApproximation(profitLikeModel, parm=Data$init, Data=Data, Iterations=1e4,
Method='BFGS', CovEst='Identity', sir=FALSE)
```

The best LA BFGS fit is given by:

```{r, eval=FALSE}
LAfit$Summary1[,1]
```

Check it out:

```{r, eval=FALSE}
profitLikeModel(LAfit$Summary1[,1],Data,makeplots=TRUE,whichcomponents=list(sersic=1))
profitLikeModel(LAfit$Summary1[,1],Data,makeplots=TRUE,whichcomponents=list(sersic=2))
profitLikeModel(LAfit$Summary1[,1],Data,makeplots=TRUE,whichcomponents=list(sersic='all'))

modeloptim=profitRemakeModelList(LAfit$Summary1[,1],Data$modellist,Data$tofit,Data$tolog)
profitEllipsePlot(Data,modeloptim,pixscale=0.2,FWHM=0.5,SBlim=26)
```

Other optimizers can be used. One particularly effective algorithm is `CMA-ES` (`Covariance Matrix Adaptation - Evolutionary Strategy`).
`CMA-ES` samples multiple points (members of a population) from the supplied priors, and then adapts the priors each iteration, shrinking the parameter space that points are sampled from to converge on the best fit.
It is a popular optimizer as it is fairly robust (but not immune) to becoming trapped in local minima while still fairly quick to converge.

First make sure that the `cmaeshpc` package is installed:

```{r, eval=FALSE}
library(devtools)
install_github('taranu/cmaeshpc')
```

It is recommended to use narrower priors than the very broad ones specified above to speed up convergence:

```{r, eval=FALSE}
library(cmaeshpc)
Data$algo.func = "CMA"
cmasigma = sigmas[which(unlist(tofit) == TRUE)]/3
cmafit = cmaeshpc(Data$init, profitLikeModel, Data=Data, control=list(maxit=1e3,
  fnscale=-1.0, sigma=cmasigma, diag.sigma=TRUE, diag.eigen=TRUE, diag.pop=TRUE,
  diag.value=TRUE, maxwalltime=Inf, trace=TRUE, stopfitness = 0, stop.tolx=1e-3*cmasigma))
profitLikeModel(cmafit$par,Data,makeplots=TRUE)
```

`CMA-ES` sometimes takes longer than `LaplaceApproximation` - depending on the convergence criterion specified by `stop.tolx` - but it usually finds a better fit, and can be run many times to avoid becoming trapped in local minima. Alternately, you may wish to use the faster `LaplaceApproximation` first, redefine your priors, and then run `CMA-ES` to search around the `LaplaceApproximation` best fit.

Now we can try a `LaplacesDemon` fit (this will take about an hour):

```{r, eval=FALSE}
Data$algo.func = "LD"

LDfit=LaplacesDemon(profitLikeModel, Initial.Values=LAfit$Summary1[,1], Data=Data,
  Iterations=1e4, Algorithm='CHARM', Thinning=1, Specs=list(alpha.star=0.44))
```

If it has converged well you will have a Summary2 structure using the ESS:

```{r, eval=FALSE}
LDfit$Summary2
```

If not you can still check Summary1:

```{r, eval=FALSE}
LDfit$Summary1
```

The global fit should be close to the initial LA fit (shown in blue in the following figures).

With any luck you have enough stationary samples to run:

```{r, eval=FALSE}
BestLD=magtri(LDfit$Posterior2, samples=500, samptype='ran')
```

Otherwise try:

```{r, eval=FALSE}
BestLD=magtri(LDfit$Posterior1, samples=1000, samptype='end')
```

We can now check our final fit:

```{r, eval=FALSE}
profitLikeModel(BestLD,Data,makeplots=TRUE,whichcomponents=list(sersic=1))
profitLikeModel(BestLD,Data,makeplots=TRUE,whichcomponents=list(sersic=2))
profitLikeModel(BestLD,Data,makeplots=TRUE,whichcomponents=list(sersic='all'))

modeloptim=profitRemakeModelList(BestLD,Data$modellist,Data$tofit,Data$tolog)
profitEllipsePlot(Data,modeloptim,pixscale=0.2,FWHM=0.5,SBlim=26)
```

In the previous examples, the resolution of the convolved model was limited by the size of the pixels, since convolution can only spread flux from pixel-to-pixel. We can do better by sampling the model and PSF on a finer grid than the pixel scale ("fine-sampling"), through interpolation for empirical PSFs.

To test this, set up the data again. This time we will fine-sample the model and PSF by a factor of 3, taking a minute to benchmark convolution methods:

```{r,eval=FALSE}
Dataf=profitSetupData(image=image, mask=mask, sigma=sigma, segim=segim, psf=psf,
  modellist=modellist, tofit=tofit, tolog=tolog, priors=priors, intervals=intervals,
  magzero=0, algo.func='LD', verbose=TRUE, nbenchmark=3L, finesample=3L)
```

Note that profitSetupData automagically fine-sampled the PSF by interpolation. Usually, brute-force convolution is faster than an FFT (which requires 2x padding to avoid artifacts), but it scales as finesample^4, so FFT is often faster with large images and/or PSFs.

Let's check to see how the fine-sampled model looks:

```{r, eval=FALSE}
profitLikeModel(BestLD,Dataf,makeplots=TRUE,whichcomponents=list(sersic='all'))
```

That doesn't look so different, but let's run `LaplaceApproximation` again to see how the best-fit parameters changed:

```{r, eval=FALSE}
Dataf$algo.func = "LA"
LAfitf=LaplaceApproximation(profitLikeModel, parm=LAfit$Summary1[,1], Data=Dataf, Iterations=1e3,
  Method='BFGS', CovEst='Identity', sir=FALSE)
```

Does the new best fit look slightly better? It should:

```{r, eval=FALSE}
profitLikeModel(LAfitf$Summary1[,1],Dataf,makeplots=TRUE)
```

Now run `LaplacesDemon` again, with fewer iterations to begin with (as it's slower to convolve):

```{r, eval=FALSE}
Dataf$algo.func = "LD"

LDfitf=LaplacesDemon(profitLikeModel, Initial.Values=LAfitf$Summary1[,1], Data=Dataf,
  Iterations=1e3, Algorithm='CHARM', Thinning=1, Specs=list(alpha.star=0.44))
```

If you run the above for 1e4 iterations (will take several hours), try comparing posteriors:

```{r, eval=FALSE}
LDfit$Summary2
LDfitf$Summary2

BestLDf=magtri(LDfit$Posterior1, samptype='end')
profitLikeModel(BestLDf,Dataf,makeplots=TRUE)
```
